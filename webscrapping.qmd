## Webscraping

This section was written by Daniel Kline, a senior at the University of 
Connecticut majoring in Math and Statistics.

# What is Web Scraping? 
- Web scraping is the process of extracting data from websites.
- Automates data gathering for analysis, research, and business intelligence.

# How Web Scraping Works
1. Send an HTTP request to a website.
2. Retrieve the HTML content of the page.
3. Parse and extract specific data.
4. Store the extracted data for further use.

# Libraires that Help
- **Python Libraries:**
   - BeautifulSoup
   - Selenium
        - Best when used with BeautifulSoup

# BeautifulSoup
   - It is a package for parsing HTML documents and web pages
   - Helps navigate, search, modify, and extract data from web pages

# Selenium
- Used for runnign automated tests for browsers or web pages
- Simulates user actions the check website functionality

# Challenges of Web Scrapping
- **Anti-Scraping Measures:** Captchas, IP blocking.
- **Website Structure Changes:** HTML structures evolve.
- **Data Cleaning:** Extracted data often needs formatting.

# Web Scraping vs. APIs
- **Web Scraping:** Extracts unstructured data from web pages.
- **APIs:** Provide structured data in a predefined format.
   - Scraping is useful when APIs are unavailable.
   - APIs are more stable and reliable.

   # Examples
```{.python}
import BeautifulSoup
import requests

url = 'https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/data_preview'

page = requests.get(url)

soup = BeautifulSoup(page.text, 'html')

print(soup)
```

# Example using NYC Crash Data {.smaller}

```{.python}
from bs4 import BeautifulSoup
import requests
import csv
import time

def get_queens_crash_data(start_date="2023-06-03", end_date="2023-06-10"):
    url = "https://data.cityofnewyork.us/resource/h9gi-nx95.json"

    start = f"{start_date}T00:00:00.000"

    if end_date == "2023-06-10":
        end = "2023-06-11T00:00:00.000" 
    else:
        end = f"{end_date}T23:59:59.999"

    params = {
        "$where": f"borough='QUEENS' AND crash_date >= '{start}' AND crash_date < '{end}'",
        "$order": "crash_date ASC",
        "$limit": 5000 
    }

    try:
        r = requests.get(url, params=params)
        if r.status_code != 200:
            print(f"Error: Status code {r.status_code}")
            return []
            
        data = r.json()
        print(f"{len(data)} crashes")
        return data
        
    except Exception as e:
        print(f"Something went wrong: {e}")
        return []

def main():
    start = "2023-06-03"
    end = "2023-06-10"

    start_time = time.time()

    crashes = get_queens_crash_data(start, end)
    
    if crashes:
        dates = {}
        for crash in crashes:
            date = crash.get('crash_date', '').split('T')[0]
            if date in dates:
                dates[date] += 1
            else:
                dates[date] = 1
        
        print("\nCrashes by date:")
        for date in sorted(dates.keys()):
            print(f"{date}: {dates[date]} crashes")
        
        injuries = 0
        deaths = 0
        for crash in crashes:
            if 'number_of_persons_injured' in crash:
                injuries += int(crash['number_of_persons_injured'])
            if 'number_of_persons_killed' in crash:
                deaths += int(crash['number_of_persons_killed'])
        
        print(f"\nTotal injured: {injuries}")
        print(f"Total killed: {deaths}")

if __name__ == "__main__":
    main()
```